{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> Scripts to build the different datasets used for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 08:35:01.712865: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-15 08:35:01.735731: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-15 08:35:01.735749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-15 08:35:01.736262: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-15 08:35:01.739678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#| hide\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "from orbit_generation.data import load_orbit_data, get_orbit_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Periods with Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All 5 periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_5p_em_dataset(data_directory: Optional[str] = '../data', \n",
    "                      output_file_path: Optional[str] = '../data/5p_dataset_em.npy') -> Tuple[np.memmap, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load orbit data and corresponding labels. Optionally, save the loaded data.\n",
    "    If the output file exists, return the data as a memory-mapped array, otherwise process and save the data.\n",
    "    After saving, the data is also returned as a memory-mapped array.\n",
    "    \"\"\"\n",
    "    # Extract file extension to determine the type\n",
    "    _, file_extension = os.path.splitext(output_file_path)\n",
    "\n",
    "    # Validate supported file types\n",
    "    if file_extension not in ['.hdf5', '.npy']:\n",
    "        raise ValueError(\"Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.\")\n",
    "\n",
    "    # Define the path for labels file\n",
    "    labels_path = f\"{os.path.splitext(output_file_path)[0]}_labels.npy\"\n",
    "\n",
    "    # Check if the combined data file already exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        # Load data as memmap\n",
    "        data_memmap = np.load(output_file_path, mmap_mode='r+') if file_extension == '.npy' else None  # Add hdf5 handling if needed\n",
    "        labels_memmap = np.load(labels_path)\n",
    "        return data_memmap, labels_memmap\n",
    "\n",
    "    # Paths to data files\n",
    "    orbits_file_path = os.path.join(data_directory, \"em_orbits.h5\")\n",
    "    features_file_path = os.path.join(data_directory, \"em_features.mat\")\n",
    "\n",
    "    # Load orbit labels\n",
    "    labels_df = get_orbit_features(features_file_path, variable_name='out_EM') \n",
    "    labels = pd.Series(labels_df['Orbit Family']).repeat(5).reset_index(drop=True).to_numpy()\n",
    "\n",
    "    # Load orbit data\n",
    "    orbit_data = load_orbit_data(orbits_file_path, dataset_path='/files/PERIODIC ORBITS')\n",
    "    reshaped_array = orbit_data.reshape(36071, 7, 5, 1500)\n",
    "    orbit_data_final = reshaped_array.transpose(0, 2, 1, 3).reshape(36071 * 5, 7, 1500)\n",
    "\n",
    "    # Save the data if an output file path is provided\n",
    "    if file_extension == '.npy':\n",
    "        np.save(output_file_path, orbit_data_final)\n",
    "        np.save(labels_path, labels)\n",
    "        data_memmap = np.load(output_file_path, mmap_mode='r+')\n",
    "\n",
    "    return data_memmap, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_1p_em_dataset(data_directory: Optional[str] = '../data',\n",
    "                      output_file_path: Optional[str] = '../data/1p_dataset_em.npy') -> Tuple[np.memmap, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load orbit data from an HDF5 file and corresponding labels from a MAT file, and optionally save the loaded data.\n",
    "    If the output file exists, it returns the data as a memory-mapped array. After saving, the data is also returned as a memory-mapped array.\n",
    "    \"\"\"\n",
    "    # Extract file extension to determine the type\n",
    "    _, file_extension = os.path.splitext(output_file_path)\n",
    "\n",
    "    # Validate supported file types\n",
    "    if file_extension not in ['.hdf5', '.npy']:\n",
    "        raise ValueError(\"Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.\")\n",
    "\n",
    "    # Define the path for labels file\n",
    "    labels_path = f\"{os.path.splitext(output_file_path)[0]}_labels.npy\"\n",
    "\n",
    "    # Check if the combined data file already exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        data_memmap = np.load(output_file_path, mmap_mode='r+') if file_extension == '.npy' else None\n",
    "        labels_memmap = np.load(labels_path)\n",
    "        return data_memmap, labels_memmap\n",
    "\n",
    "    # Paths to data files\n",
    "    orbits_file_path = os.path.join(data_directory, \"em_orbits.h5\")\n",
    "    features_file_path = os.path.join(data_directory, \"em_features.mat\")\n",
    "\n",
    "    # Load orbit labels\n",
    "    labels_df = get_orbit_features(features_file_path, variable_name='out_EM') \n",
    "    labels = labels_df['Orbit Family'].to_numpy()\n",
    "\n",
    "    # Load orbit data\n",
    "    orbit_data = load_orbit_data(orbits_file_path, dataset_path='/files/PERIODIC ORBITS')\n",
    "    reshaped_orbit_data = orbit_data[:, :, :1500]\n",
    "\n",
    "    # Save the data if an output file path is provided\n",
    "    if file_extension == '.npy':\n",
    "        np.save(output_file_path, reshaped_orbit_data)\n",
    "        np.save(labels_path, labels)\n",
    "        data_memmap = np.load(output_file_path, mmap_mode='r+')\n",
    "\n",
    "    return data_memmap, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Period for each Orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_sp_em_dataset(data_directory: Optional[str] = '../data', \n",
    "                      output_file_path: Optional[str] = '../data/sp_dataset_em.npy') -> Tuple[np.memmap, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load orbit data and corresponding labels based on the specified number of periods per orbit.\n",
    "    If the output file exists, return the data as a memory-mapped array, otherwise process and save the data.\n",
    "    After saving, the data is also returned as a memory-mapped array.\n",
    "    \"\"\"\n",
    "    # Extract file extension to determine the type\n",
    "    _, file_extension = os.path.splitext(output_file_path)\n",
    "\n",
    "    # Validate supported file types\n",
    "    if file_extension not in ['.hdf5', '.npy']:\n",
    "        raise ValueError(\"Unsupported file extension. Supported extensions are '.hdf5' or '.npy'.\")\n",
    "\n",
    "    # Define the path for labels file\n",
    "    labels_path = f\"{os.path.splitext(output_file_path)[0]}_labels.npy\"\n",
    "\n",
    "    # Check if the combined data file already exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        data_memmap = np.load(output_file_path, mmap_mode='r+') if file_extension == '.npy' else None\n",
    "        labels_memmap = np.load(labels_path)\n",
    "        return data_memmap, labels_memmap\n",
    "\n",
    "    # Paths to data files\n",
    "    orbits_file_path = os.path.join(data_directory, \"em_orbits.h5\")\n",
    "    features_file_path = os.path.join(data_directory, \"em_features.mat\")\n",
    "    periods_file_path = os.path.join(data_directory, \"em_periods.npy\")\n",
    "\n",
    "    # Load orbit labels\n",
    "    labels_df = get_orbit_features(features_file_path, variable_name='out_EM') \n",
    "    labels = labels_df['Orbit Family'].to_numpy()\n",
    "\n",
    "    # Load number of periods per orbit\n",
    "    periods_per_orbit = np.load(periods_file_path)\n",
    "\n",
    "    # Load orbit data\n",
    "    with h5py.File(orbits_file_path, 'r') as file:\n",
    "        orbit_data = np.array(file['/files/PERIODIC ORBITS'])\n",
    "\n",
    "    # Initialize the result array and label list\n",
    "    orbit_data_final = []\n",
    "    final_labels = []\n",
    "\n",
    "    # Iterate over the orbits and periods array\n",
    "    for index, num_periods in enumerate(periods_per_orbit):\n",
    "        for period_idx in range(num_periods):\n",
    "            start_idx = 1500 * period_idx\n",
    "            end_idx = start_idx + 1500\n",
    "            orbit_slice = orbit_data[index, :, start_idx:end_idx]\n",
    "            orbit_data_final.append(orbit_slice)\n",
    "            final_labels.append(labels[index])\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    orbit_data_final = np.stack(orbit_data_final)  # This ensures a uniform 3D array\n",
    "\n",
    "    # Save the data if an output file path is provided\n",
    "    if file_extension == '.npy':\n",
    "        np.save(output_file_path, orbit_data_final)\n",
    "        np.save(labels_path, final_labels)\n",
    "        data_memmap = np.load(output_file_path, mmap_mode='r+')\n",
    "        final_labels = np.load(labels_path)\n",
    "\n",
    "    return data_memmap, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, labels = get_sp_em_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Period for each Orbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "include labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_sp_fixed_em_dataset(data_directory: Optional[str] = '../data', \n",
    "                            output_data_path: Optional[str] = '../data/em_periods.npy', \n",
    "                            output_periods_path: Optional[str] = '../data/em_periods.npy') -> Tuple[Dict[int, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load orbit data from an HDF5 file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the default data directory based on the script's location if not provided\n",
    "    if data_directory is None:\n",
    "        data_directory = os.path.join(os.path.dirname(__file__), \"Data\")\n",
    "    \n",
    "    # Define the file path for the orbit data\n",
    "    file_path = os.path.join(data_directory, \"em_orbits_dt_0_01.h5\")\n",
    "    \n",
    "    # Initialize a dictionary to hold data from all datasets\n",
    "    orbit_data = {}\n",
    "    \n",
    "    # Open the HDF5 file and read data from each dataset\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # Iterate through each dataset in the HDF5 file\n",
    "        for name in file:\n",
    "            dataset = np.array(file[name])\n",
    "            if name.isdigit():\n",
    "                orbit_data[int(name)] = dataset\n",
    "            else:\n",
    "                orbit_data[name] = dataset\n",
    "    \n",
    "    # Extract the periods\n",
    "    periods = orbit_data.get('prop_periods')[0] if 'prop_periods' in orbit_data else None\n",
    "\n",
    "    # Save the entire orbit data if an output path is provided\n",
    "    if output_data_path:\n",
    "        if output_data_path.endswith('.npy'):\n",
    "            np.save(output_data_path, orbit_data)\n",
    "        elif output_data_path.endswith('.h5'):\n",
    "            with h5py.File(output_data_path, 'w') as h5_file:\n",
    "                for key, data in orbit_data.items():\n",
    "                    h5_file.create_dataset(key, data=data)\n",
    "\n",
    "    # Save periods data if an output path is provided\n",
    "    if output_periods_path:\n",
    "        if output_periods_path.endswith('.npy'):\n",
    "            np.save(output_periods_path, periods)\n",
    "        elif output_periods_path.endswith('.h5'):\n",
    "            with h5py.File(output_periods_path, 'w') as h5_file:\n",
    "                h5_file.create_dataset('periods', data=periods)\n",
    "\n",
    "    return orbit_data, periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
